{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TinyTroupe with Ollama\n",
    "\n",
    "This example demonstrates how to use TinyTroupe with local models via Ollama integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running this notebook, make sure Ollama is installed and running on your system.\n",
    "You can install Ollama from: https://ollama.com/\n",
    "\n",
    "You'll also need to pull the model you want to use. For this example, we'll use \"long-gemma\":\n",
    "\n",
    "```bash\n",
    "ollama pull long-gemma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing TinyTroupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tinytroupe as tt\n",
    "from tinytroupe.openai_utils import configure, client\n",
    "\n",
    "# Enable more verbose logging for debugging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"tinytroupe\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching to Ollama\n",
    "\n",
    "By default, TinyTroupe will use the API_TYPE configured in your config.ini. To explicitly use Ollama for this session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Force using Ollama for this session\n",
    "force_api_type(\"ollama\")\n",
    "\n",
    "# Verify we're using the Ollama client\n",
    "print(f\"Using client: {client().__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Persona\n",
    "\n",
    "Let's create a simple persona that will use our local model via Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a persona\n",
    "local_persona = tt.Persona(\n",
    "    name=\"LocalExpert\",\n",
    "    backstory=\"I am an AI assistant running on a local model through Ollama. I help answer questions using locally available computing resources.\",\n",
    "    traits=[\"helpful\", \"concise\", \"knowledgeable\"],\n",
    "    role=\"assistant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having a conversation with the local model\n",
    "\n",
    "Now let's have a simple conversation with our local model-powered persona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a conversation\n",
    "conversation = tt.Conversation()\n",
    "\n",
    "# Add the persona to the conversation\n",
    "conversation.add_persona(local_persona)\n",
    "\n",
    "# Start the conversation with a question\n",
    "response = conversation.send_message(\n",
    "    sender=\"user\",\n",
    "    content=\"Hello, can you tell me what are the advantages of using local models like yourself instead of cloud-based LLMs?\",\n",
    "    target=\"LocalExpert\"\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(f\"LocalExpert says: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "response = conversation.send_message(\n",
    "    sender=\"user\",\n",
    "    content=\"What are the trade-offs between using local models versus cloud-based models?\",\n",
    "    target=\"LocalExpert\"\n",
    ")\n",
    "\n",
    "print(f\"LocalExpert says: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching back to OpenAI\n",
    "\n",
    "If you need to switch back to using OpenAI models, you can do so at any time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Switch back to OpenAI\n",
    "force_api_type(\"openai\")\n",
    "\n",
    "# Verify we're using the OpenAI client\n",
    "print(f\"Now using client: {client().__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This example demonstrated how to use TinyTroupe with local models via Ollama. The integration allows you to:\n",
    "\n",
    "1. Switch between cloud-based models (OpenAI, Azure) and local models (Ollama)\n",
    "2. Use the same TinyTroupe API for both cloud and local models\n",
    "3. Leverage the privacy and cost benefits of local models with the same features\n",
    "\n",
    "You can customize the Ollama integration by modifying the `[Ollama]` section in your `config.ini` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
