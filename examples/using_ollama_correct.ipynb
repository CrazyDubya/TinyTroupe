{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TinyTroupe with Ollama\n",
    "\n",
    "This example demonstrates how to use TinyTroupe with local models via Ollama integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running this notebook, make sure Ollama is installed and running on your system.\n",
    "You can install Ollama from: https://ollama.com/\n",
    "\n",
    "You'll also need to pull the model you want to use. For this example, we'll use \"long-gemma\":\n",
    "\n",
    "```bash\n",
    "ollama pull long-gemma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing TinyTroupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from tinytroupe.agent.tiny_person import TinyPerson\n",
    "from tinytroupe.environment.tiny_world import TinyWorld\n",
    "from tinytroupe.openai_utils import configure, client\n",
    "\n",
    "# Enable more verbose logging for debugging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"tinytroupe\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching to Ollama\n",
    "\n",
    "By default, TinyTroupe will use the API_TYPE configured in your config.ini. To explicitly use Ollama for this session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure to use Ollama for this session\n",
    "configure(api_type=\"ollama\")\n",
    "\n",
    "# Verify we're using the Ollama client\n",
    "print(f\"Using client: {client().__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple World and Persona\n",
    "\n",
    "Let's create a simple persona that will use our local model via Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a world\n",
    "world = TinyWorld()\n",
    "\n",
    "# Create a persona\n",
    "local_persona = TinyPerson(\n",
    "    name=\"LocalExpert\",\n",
    "    backstory=\"I am an AI assistant running on a local model through Ollama. I help answer questions using locally available computing resources.\",\n",
    "    traits=[\"helpful\", \"concise\", \"knowledgeable\"],\n",
    "    role=\"assistant\",\n",
    "    world=world\n",
    ")\n",
    "\n",
    "# Add the persona to the world\n",
    "world.add_agent(local_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having a conversation with the local model\n",
    "\n",
    "Now let's have a simple conversation with our local model-powered persona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start the conversation with a question\n",
    "world.broadcast(\"Hello, can you tell me what are the advantages of using local models like yourself instead of cloud-based LLMs?\")\n",
    "\n",
    "# The response will appear in the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "world.broadcast(\"What are the trade-offs between using local models versus cloud-based models?\")\n",
    "\n",
    "# The response will appear in the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching back to OpenAI\n",
    "\n",
    "If you need to switch back to using OpenAI models, you can do so at any time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Switch back to OpenAI\n",
    "configure(api_type=\"openai\")\n",
    "\n",
    "# Verify we're using the OpenAI client\n",
    "print(f\"Now using client: {client().__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This example demonstrated how to use TinyTroupe with local models via Ollama. The integration allows you to:\n",
    "\n",
    "1. Switch between cloud-based models (OpenAI, Azure) and local models (Ollama)\n",
    "2. Use the same TinyTroupe API for both cloud and local models\n",
    "3. Leverage the privacy and cost benefits of local models with the same features\n",
    "\n",
    "You can customize the Ollama integration by modifying the `[Ollama]` section in your `config.ini` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
