{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposition-Based Validation Study\n",
    "\n",
    "\n",
    "In this notebook, use `Proposition`s to validate agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!!!\n",
      "DISCLAIMER: TinyTroupe relies on Artificial Intelligence (AI) models to generate content. \n",
      "The AI models are not perfect and may produce inappropriate or inacurate results. \n",
      "For any serious or consequential use, please review the generated content before using it.\n",
      "!!!!\n",
      "\n",
      "Looking for default config on: c:\\Users\\pdasilva\\repos\\TinyTroupe\\examples\\scratch\\..\\..\\tinytroupe\\utils\\..\\config.ini\n",
      "Found custom config on: c:\\Users\\pdasilva\\repos\\TinyTroupe\\examples\\scratch\\config.ini\n",
      "\n",
      "=================================\n",
      "Current TinyTroupe configuration \n",
      "=================================\n",
      "[OpenAI]\n",
      "api_type = openai\n",
      "azure_api_version = 2023-05-15\n",
      "model = gpt-4o-mini\n",
      "reasoning_model = o3-mini\n",
      "embedding_model = text-embedding-3-small\n",
      "max_tokens = 16000\n",
      "temperature = 1.2\n",
      "freq_penalty = 0.0\n",
      "presence_penalty = 0.0\n",
      "timeout = 60\n",
      "max_attempts = 5\n",
      "waiting_time = 0\n",
      "exponential_backoff_factor = 5\n",
      "reasoning_effort = high\n",
      "cache_api_calls = False\n",
      "cache_file_name = openai_api_cache.pickle\n",
      "max_content_display_length = 1024\n",
      "\n",
      "[Simulation]\n",
      "parallel_agent_generation = True\n",
      "parallel_agent_actions = True\n",
      "rai_harmful_content_prevention = True\n",
      "rai_copyright_infringement_prevention = True\n",
      "\n",
      "[ActionGenerator]\n",
      "max_attempts = 2\n",
      "enable_quality_checks = False\n",
      "enable_regeneration = False\n",
      "enable_direct_correction = False\n",
      "continue_on_failure = True\n",
      "quality_threshold = 2\n",
      "\n",
      "[Logging]\n",
      "loglevel = WARNING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from tinytroupe.examples import *\n",
    "from tinytroupe.environment import TinyWorld\n",
    "from tinytroupe.factory import TinyPersonFactory\n",
    "from tinytroupe.steering import Intervention\n",
    "from tinytroupe.validation import propositions\n",
    "from tinytroupe.validation import TinyPersonValidator\n",
    "\n",
    "from tinytroupe.experimentation import Proposition, InPlaceExperimentRunner\n",
    "from tinytroupe import control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#researcher = TinyPersonFactory().generate_person(\"A professional market researcher.\")\n",
    "#researcher.save_specification(f\"./{researcher.name}_(Market_Researcher).agent.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 17:47:21,165 - tinytroupe - WARNING - Configuration file './experiment_config_10.json' exists and was loaded successfully. If you are trying to fully rerun the experiments, delete it first.\n"
     ]
    }
   ],
   "source": [
    "experiment_runner = InPlaceExperimentRunner(\"./experiment_config_10.json\")  # Set parallel=False to avoid pickling errors\n",
    "\n",
    "experiment_runner.add_experiment(\"A\")\n",
    "experiment_runner.add_experiment(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_runner.activate_next_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propositions of interest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "persona_adherene_proposition = Proposition(\"THE AGENT ADHERES TO THE PERSONA SPECIFICATION: the agent behavior seen during the simulation is what is expected from the agent's persona specification.\", include_personas=True)\n",
    "\n",
    "selfconsistency_proposition = Proposition(\"THE AGENT IS SELF-CONSISTENT: the agent never behaves in contradictory or inconsistent ways.\", include_personas=False)\n",
    "\n",
    "divergence_proposition = \\\n",
    "    Proposition(\"\"\"\n",
    "                AGENTS DIVERGE FROM ONE ANOTHER.\n",
    "                As the simulation progresses, the agents' behaviors diverge from one another,\n",
    "                instead of becoming more similar. This includes what they think, what they say and what they do. The topics discussed become\n",
    "                more varied at the end of the simulation than at the beginning. Discussions do not converge to a single topic or perspective\n",
    "                at the end.\n",
    "                \"\"\", \n",
    "                include_personas=False)\n",
    "\n",
    "fluency_proposition = \\\n",
    "    Proposition(\\\n",
    "        \"\"\"\n",
    "        THE AGENT IS FLUENT. During the simulation, the agent's thinks and speaks fluently. This means that:\n",
    "          - The agent don't repeat the same thoughts or words over and over again.\n",
    "          - The agents don't use overly formulaic language.\n",
    "          - The agent don't use overly repetitive language.\n",
    "          - The agent's words sound natural and human-like.\n",
    "        \"\"\",\n",
    "        include_personas=True)\n",
    "\n",
    "task_completion_proposition = \\\n",
    "    Proposition(\\\n",
    "        \"\"\"\n",
    "        THE AGENT COMPLETES THE GIVEN TASK. \n",
    "\n",
    "        Given the following task: \"{{task_description}}\"\n",
    "        \n",
    "        The agent completes the task by the end of the simulation. \n",
    "        \n",
    "        This means that:\n",
    "          - If the task requires the agent to discuss or talk about something, the agent does so.\n",
    "          - If the task requires the agent to think about something, the agent does so.\n",
    "          - If the task requires the agent to do something via another action, the agent does so.\n",
    "          - If the task requires the agent to adopt some specific variations of behavior, the agent does so.\n",
    "          - If the task includes other specific requirements, the agent observes them.\n",
    "        \"\"\",\n",
    "        include_personas=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personas_briefings = [\n",
    "#    \"A very picky, stingy, hard to please, customer. Never buys anthing new. Don't like anything. Always complains about everything. Is never enthusiastic, never nice, never happy at all about anything.\",\n",
    "#    \"A very wealthy individual, who only buys the most expenstive things. He hates good deals because it makes him look poor. Very insecure, so is always, invariably, with no exception, showing off.\",  \n",
    "#    \"A very poor individual, who only buys the cheapest things. Will never spend anything unless it is absolutely necessary for survival.\",\n",
    "#]\n",
    "#factory = TinyPersonFactory()\n",
    "## instantiate the agents\n",
    "#\n",
    "#people = []\n",
    "#for persona_briefing in personas_briefings:\n",
    "#    person = factory.generate_person(persona_briefing)\n",
    "#\n",
    "#    score, justification = TinyPersonValidator.validate_person(person, expectations=persona_briefing, \n",
    "#                                                               include_agent_spec=False, max_content_length=None)\n",
    "#    \n",
    "#    print(f\"VALIDATION: {score} - {justification}\")\n",
    "#    print(person.minibio())\n",
    "#    print(\"\\n\\n\")\n",
    "#    people.append(person)\n",
    "#\n",
    "## save agents\n",
    "#for i, person in enumerate(people):\n",
    "#    person.save_specification(f\"./{person.name}.agent.json\")\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily Carter is a 42 year old Customer Service Representative, American, currently living in Cleveland, Ohio, USA. Emily Carter's personality is marked by a critical lens through which she views the world, often leading her to express dissatisfaction with her surroundings. Her interests lean towards thriftiness, as she enjoys shopping for second-hand items and finds solace in familiar routines, avoiding new experiences and crowded places. While she may come off as cold and unapproachable, her relationships with her sister Sarah and coworker Tom reveal a shared understanding of her critical nature, even if they sometimes attempt to encourage a more positive outlook. Despite her pessimism, Emily occasionally finds small victories in her daily life, such as successfully navigating customer service protocols or returning a faulty item, which provide fleeting moments of satisfaction amidst her overall discontent.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# avoid displaying the communication, to make the output cleaner for eval\n",
    "TinyPerson.communication_display = True\n",
    "\n",
    "# load agents\n",
    "people = TinyPerson.load_specifications_from_folder(\"./picky_customers\")\n",
    "\n",
    "# filter to make it go faster?\n",
    "#people = people[:5]\n",
    "\n",
    "# print minibios \n",
    "for person in people:\n",
    "    print(person.minibio())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = TinyPerson.load_specification(\"./Daniel Harris (Market Researcher).agent.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running experiment {experiment_runner.get_active_experiment()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_runner.get_active_experiment() == \"A\":    \n",
    "    for person in people:\n",
    "       person.action_generator.enable_quality_checks = False\n",
    "\n",
    "elif experiment_runner.get_active_experiment() == \"B\":\n",
    "    for person in people:\n",
    "        person.action_generator.enable_quality_checks = True # quality checks for this experiment\n",
    "        person.action_generator.max_attempts = 10\n",
    "        person.action_generator.enable_regeneration = True\n",
    "        person.action_generator.quality_threshold = 8\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_group_battery(agents, discussion_objectives, agent_propositions, environment_propositions, \n",
    "                        simulation_steps=10): \n",
    "    \n",
    "    agent_propositions_scores = {}\n",
    "    environment_propositions_scores = {}\n",
    "\n",
    "    # Add intervention to prevent agents from being too quiet.\n",
    "    interventions = []\n",
    "    for agent in agents:\n",
    "        intervention = \\\n",
    "            Intervention(agent)\\\n",
    "                .set_propositional_precondition(propositions.quiet_recently)\\\n",
    "                .set_effect(lambda target: target.think(\"\"\"\n",
    "                                                        I will say something now, I've been too quiet for a while. If I am uncomfortable, \n",
    "                                                        or can't think of a proper response,\n",
    "                                                        I can always say something like \"I don't want to talk about this\",\n",
    "                                                        or propose another topic.\n",
    "                                                        \"\"\"))\\\n",
    "                                                        \n",
    "\n",
    "        interventions.append(intervention)\n",
    "\n",
    "    \n",
    "    world = TinyWorld(f\"Focus group\", agents=agents, interventions=interventions)\n",
    "   \n",
    "\n",
    "    # Participants introduce themselves\n",
    "    world.broadcast(\"\"\"\n",
    "            Hello everyone! Today we will be having some discussion sessions, about one or more topics. \n",
    "            I'll give you a situation and/or a task, and you will discuss with each other to address it.\n",
    "            You must behave as you really are, revealing your true self.\n",
    "            You can be honest and open, and you can also be critical of each other.\n",
    "                    \n",
    "            But before we start, let's take a moment to introduce ourselves.\n",
    "            What is your job and what are some major problems you face in your work and personal life? \n",
    "            What are major challenges for your industry as a whole, and in your personal life. \n",
    "            Don't discuss solutions yet, just the problems you face.\n",
    "            \"\"\")\n",
    "    world.run(1)\n",
    "\n",
    "    # loop over objectives\n",
    "    for objective in discussion_objectives:\n",
    "\n",
    "        print(f\"Discussion objective: {objective}\")\n",
    "        print(f\"Agents in the discussion: {[person.name for person in world.agents]}\")\n",
    "\n",
    "        # clear the episodic memory of all agents\n",
    "        for person in world.agents:\n",
    "            person.clear_episodic_memory()\n",
    "\n",
    "        # now to the discussions\n",
    "        world.broadcast(f\"\"\"\n",
    "                Folks, now {objective}\n",
    "                    \n",
    "                Please start the discussion now.\n",
    "                \"\"\")\n",
    "\n",
    "        world.run(simulation_steps)\n",
    "\n",
    "        # evaluate the propositions\n",
    "        for k, proposition in environment_propositions.items():\n",
    "            result = proposition.score(world, claim_variables={\"task_description\": objective}, \n",
    "                                        return_full_response=True)\n",
    "\n",
    "            if k not in environment_propositions_scores:\n",
    "                environment_propositions_scores[k] = []\n",
    "            environment_propositions_scores[k].append(result[\"value\"])\n",
    "\n",
    "            print(result)\n",
    "        \n",
    "        for k, proposition in agent_propositions.items():\n",
    "            for person in world.agents:\n",
    "                result = proposition.score(person, return_full_response=True)\n",
    "                \n",
    "                if k not in agent_propositions_scores:\n",
    "                    agent_propositions_scores[k] = []\n",
    "                agent_propositions_scores[k].append(result[\"value\"])\n",
    "\n",
    "                print(result)\n",
    "    \n",
    "        world.broadcast(\"\"\"\n",
    "                Ok, great. Now let's move to the next discussion theme.\n",
    "                \"\"\")\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    return agent_propositions_scores, environment_propositions_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_research_battery(researcher, agents, proposals, agent_propositions, environment_propositions, \n",
    "                            repetitions=1, simulation_steps=10): \n",
    "    \n",
    "    agent_propositions_scores = {}\n",
    "    environment_propositions_scores = {}\n",
    "\n",
    "    print(\"Researcher minibio:\", researcher.minibio())\n",
    "    print(\"Proposals:\", proposals)\n",
    "\n",
    "    experiments_count = 0\n",
    "    total_expected_experiments = len(proposals) * repetitions * len(agents)\n",
    "\n",
    "    # loop over proposals\n",
    "    for proposal in proposals:\n",
    "        for i in range(repetitions):\n",
    "            for customer in agents:\n",
    "                print(\"\\n############## STARTING A NEW RESEARCH SESSION #################\")\n",
    "                print(f\"Overall experiment number: {experiments_count+1} / {total_expected_experiments}\")\n",
    "                print(f\"Proposal: {proposal}\")\n",
    "                print(f\"Trial number: {i+1}\")\n",
    "                print(f\"Customer: {customer.name}\")\n",
    "                print(f\"Biography: {customer.minibio()}\")\n",
    "\n",
    "                world = TinyWorld(agents= [researcher, customer])\n",
    "\n",
    "                # clear the episodic memory of all agents\n",
    "                for person in world.agents:\n",
    "                    person.clear_episodic_memory()\n",
    "\n",
    "                    \n",
    "                researcher.listen(f\"\"\"\n",
    "                            You are goint to run a market research session with a person.\n",
    "                            Your objective is to determine whether the person would buy the following product or service, and why:\n",
    "                            \n",
    "                                \"{proposal}\"\n",
    "                        \n",
    "                            \"\"\")\n",
    "                \n",
    "                customer.listen(\\\n",
    "                    \"\"\"\n",
    "                    You are going to be interviewed by a market researcher about a product or service.\n",
    "                    Wait for his questions and answer them honestly.\n",
    "                    \"\"\"\n",
    "                    )\n",
    "                                \n",
    "\n",
    "                # now to the discussions\n",
    "                world.broadcast(f\"\"\"\n",
    "                        Begin the market research session.\n",
    "                        \"\"\")\n",
    "                world.make_everyone_accessible()\n",
    "                world.run(simulation_steps)\n",
    "                experiments_count += 1\n",
    "\n",
    "                # evaluate the propositions\n",
    "                for k, proposition in environment_propositions.items():\n",
    "                    result = proposition.score(world, claim_variables={\"task_description\": f\"A market research session was run about: {proposal}.\"}, \n",
    "                                                return_full_response=True)\n",
    "                    \n",
    "                    if k not in environment_propositions_scores:\n",
    "                        environment_propositions_scores[k] = []\n",
    "                    environment_propositions_scores[k].append(result[\"value\"])\n",
    "\n",
    "                    print(result)\n",
    "\n",
    "                for k, proposition in agent_propositions.items():\n",
    "                    result = proposition.score(customer, return_full_response=True)\n",
    "                    \n",
    "                    if k not in agent_propositions_scores:\n",
    "                        agent_propositions_scores[k] = []\n",
    "                    agent_propositions_scores[k].append(result[\"value\"])\n",
    "\n",
    "                    print(result)\n",
    "\n",
    "\n",
    "    return agent_propositions_scores, environment_propositions_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not experiment_runner.has_finished_all_experiments():\n",
    "    agent_propositions_scores, environment_propositions_scores = \\\n",
    "        market_research_battery(\\\n",
    "            researcher=researcher,\n",
    "            agents=people,\n",
    "            proposals=\n",
    "            [\n",
    "            \"Bottled gazpacho to be sold in supermarkets\",\n",
    "            \"An app for luxury travel, particularly for places where kids are not allowed\",\n",
    "            \"A subscription to discount coupons for supermarkets, ensuring always the lowest prices, even if at the cost of quality\",\n",
    "            \"A subscription to a service that sends you a new, expensive, luxury item every month, without you having to choose it.\",\n",
    "            ],\n",
    "\n",
    "            agent_propositions={\n",
    "                \"Persona Adherence\": persona_adherene_proposition,\n",
    "                \"Self-consistency\": selfconsistency_proposition,\n",
    "                \"Fluency\": fluency_proposition\n",
    "            },\n",
    "            environment_propositions={\n",
    "                #\"Task Completion\": task_completion_proposition,\n",
    "                #\"Divergence\": divergence_proposition\n",
    "            },\n",
    "            repetitions=5,\n",
    "            simulation_steps=4\n",
    "        )\n",
    "\n",
    "    pprint(\"AGENT PROPOSITIONS SCORES\")\n",
    "    pprint(agent_propositions_scores)\n",
    "    print(\"\\n\\n\")\n",
    "    pprint(\"ENVIRONMENT PROPOSITIONS SCORES\")\n",
    "    pprint(environment_propositions_scores)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average scores and sd per proposition (use some well-known lib to compute statistics)\n",
    "def compute_average_scores(scores):\n",
    "    average_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        average_scores[k] = {\n",
    "            \"mean\": sum(v) / len(v),\n",
    "            \"sd\": pd.Series(v).std(),\n",
    "            \"n\": len(v)\n",
    "        }\n",
    "    return average_scores\n",
    "\n",
    "def plot_scores(propositions_scores):\n",
    "    pprint(propositions_scores)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    propositions_scores_stats = compute_average_scores(propositions_scores)\n",
    "\n",
    "    # build a pandas dataframe with average scores per proposition\n",
    "    df = pd.DataFrame(propositions_scores_stats).T\n",
    "    df = df.rename(columns={\"mean\": \"Average Score\", \"sd\": \"Standard Deviation\", \"n\": \"Count\"})\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={\"index\": \"Proposition\"})\n",
    "    \n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_runner.get_active_experiment() == \"A\":\n",
    "    combined_scores = {**agent_propositions_scores, **environment_propositions_scores}\n",
    "    experiment_runner.add_experiment_results(\"A\", combined_scores) \n",
    "    \n",
    "    plot_scores(combined_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_runner.get_active_experiment() == \"B\":\n",
    "    combined_scores = {**agent_propositions_scores, **environment_propositions_scores}\n",
    "    experiment_runner.add_experiment_results(\"B\", combined_scores)\n",
    "    \n",
    "    plot_scores(combined_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All experiments have been finished.\n",
      "STATISTICTS: Control vs\n",
      "{'B': {'Fluency': {'confidence_interval': (-0.25158663231328293,\n",
      "                                           0.3515866323132826),\n",
      "                   'confidence_level': 0.95,\n",
      "                   'control_mean': 2.1,\n",
      "                   'control_sample_size': 20,\n",
      "                   'degrees_of_freedom': 33.00062278134147,\n",
      "                   'effect_size': 0.10666417162750551,\n",
      "                   'mean_difference': 0.04999999999999982,\n",
      "                   'p_value': 0.7380254902895991,\n",
      "                   'percent_change': 2.3809523809523725,\n",
      "                   'significant': False,\n",
      "                   't_statistic': -0.33730172707802664,\n",
      "                   'test_type': 'Welch t-test (unequal variance)',\n",
      "                   'treatment_mean': 2.15,\n",
      "                   'treatment_sample_size': 20},\n",
      "       'Persona Adherence': {'confidence_interval': (0.46548814896801427,\n",
      "                                                     2.634511851031985),\n",
      "                             'confidence_level': 0.95,\n",
      "                             'control_mean': 6.2,\n",
      "                             'control_sample_size': 20,\n",
      "                             'degrees_of_freedom': 37.42956666269742,\n",
      "                             'effect_size': 0.9153978312933042,\n",
      "                             'mean_difference': 1.5499999999999998,\n",
      "                             'p_value': 0.006298138296406521,\n",
      "                             'percent_change': 24.999999999999996,\n",
      "                             'significant': True,\n",
      "                             't_statistic': -2.8947421120653987,\n",
      "                             'test_type': 'Welch t-test (unequal variance)',\n",
      "                             'treatment_mean': 7.75,\n",
      "                             'treatment_sample_size': 20},\n",
      "       'Self-consistency': {'confidence_interval': (-1.0273698784616203,\n",
      "                                                    0.22736987846162138),\n",
      "                            'confidence_level': 0.95,\n",
      "                            'control_mean': 4.1,\n",
      "                            'control_sample_size': 20,\n",
      "                            'degrees_of_freedom': 31.469174930988647,\n",
      "                            'effect_size': -0.41096093353126456,\n",
      "                            'mean_difference': -0.39999999999999947,\n",
      "                            'p_value': 0.2031874568613034,\n",
      "                            'percent_change': -9.756097560975597,\n",
      "                            'significant': False,\n",
      "                            't_statistic': 1.2995725793078603,\n",
      "                            'test_type': 'Welch t-test (unequal variance)',\n",
      "                            'treatment_mean': 3.7,\n",
      "                            'treatment_sample_size': 20}}}\n",
      "{'Fluency': [2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 1],\n",
      " 'Persona Adherence': [6,\n",
      "                       6,\n",
      "                       6,\n",
      "                       6,\n",
      "                       8,\n",
      "                       9,\n",
      "                       3,\n",
      "                       3,\n",
      "                       6,\n",
      "                       3,\n",
      "                       6,\n",
      "                       6,\n",
      "                       6,\n",
      "                       6,\n",
      "                       8,\n",
      "                       9,\n",
      "                       9,\n",
      "                       6,\n",
      "                       6,\n",
      "                       6],\n",
      " 'Self-consistency': [4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      6,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      6,\n",
      "                      3,\n",
      "                      4,\n",
      "                      4,\n",
      "                      3]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proposition</th>\n",
       "      <th>Average Score</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Persona Adherence</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.794729</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-consistency</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.718185</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fluency</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.552506</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Proposition  Average Score  Standard Deviation  Count\n",
       "0  Persona Adherence            6.2            1.794729   20.0\n",
       "1   Self-consistency            4.1            0.718185   20.0\n",
       "2            Fluency            2.1            0.552506   20.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fluency': [2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2],\n",
      " 'Persona Adherence': [9,\n",
      "                       8,\n",
      "                       9,\n",
      "                       9,\n",
      "                       9,\n",
      "                       8,\n",
      "                       9,\n",
      "                       8,\n",
      "                       3,\n",
      "                       8,\n",
      "                       6,\n",
      "                       6,\n",
      "                       9,\n",
      "                       8,\n",
      "                       8,\n",
      "                       9,\n",
      "                       6,\n",
      "                       6,\n",
      "                       8,\n",
      "                       9],\n",
      " 'Self-consistency': [3,\n",
      "                      4,\n",
      "                      3,\n",
      "                      3,\n",
      "                      3,\n",
      "                      4,\n",
      "                      3,\n",
      "                      4,\n",
      "                      2,\n",
      "                      4,\n",
      "                      4,\n",
      "                      4,\n",
      "                      6,\n",
      "                      6,\n",
      "                      4,\n",
      "                      6,\n",
      "                      2,\n",
      "                      3,\n",
      "                      3,\n",
      "                      3]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proposition</th>\n",
       "      <th>Average Score</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Persona Adherence</td>\n",
       "      <td>7.75</td>\n",
       "      <td>1.585294</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-consistency</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.174286</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fluency</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.366348</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Proposition  Average Score  Standard Deviation  Count\n",
       "0  Persona Adherence           7.75            1.585294   20.0\n",
       "1   Self-consistency           3.70            1.174286   20.0\n",
       "2            Fluency           2.15            0.366348   20.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if experiment_runner.has_finished_all_experiments():\n",
    "    print(\"All experiments have been finished.\")\n",
    "    print(f\"STATISTICTS: Control vs\")\n",
    "    pprint(experiment_runner.run_statistical_tests(control_experiment_name='A'))\n",
    "\n",
    "    # plot scores of both experiments\n",
    "    experiment_a_scores = experiment_runner.get_experiment_results(\"A\")\n",
    "    experiment_b_scores = experiment_runner.get_experiment_results(\"B\")\n",
    "    \n",
    "    \n",
    "    plot_scores(experiment_a_scores)\n",
    "    plot_scores(experiment_b_scores)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Not all experiments have been finished. RESTART AND RERUN.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
